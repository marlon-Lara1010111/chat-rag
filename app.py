from flask import Flask, request, jsonify
from flask_cors import CORS
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
import os
import re
from dotenv import load_dotenv

# Configura√ß√µes
load_dotenv()
app = Flask(__name__)
CORS(app)

# Modelo de respostas
RESPONSE_TEMPLATES = {
    "greetings": {
        r"(?i)^(oi|ol√°|ola|e a√≠|hey|hello)$": "üëã Ol√°! Sou seu assistente do SENAI. Como posso ajudar?",
        r"(?i)^(quem √© voc√™|quem te criou)$": "ü§ñ Sou um assistente IA especializado em documentos SENAI com RAG (Ollama + LangChain)",
        r"(?i)^(bom dia|boa tarde|boa noite)$": lambda m: f"‚òÄÔ∏è {m[0].title()}! Pergunte sobre o documento." if "noite" not in m[0] else f"üåô {m[0].title()}! Estou aqui para ajudar.",
        r"(?i)^(tchau|at√© logo|flw|vlw)$": "üëã At√© logo! Qualquer d√∫vida estou aqui!",
        r"(?i)^(obrigado|valeu|agrade√ßo)$": "üòä Por nada! Fico feliz em ajudar!"
    },
    "fallback": """üîç N√£o encontrei isso no documento. Posso responder sobre:
- **Estrat√©gias de desenho curricular**
- **Perfis profissionais**
- **Itiner√°rios formativos**

Experimente perguntar sobre um desses t√≥picos!"""
}

def load_and_split_pdf():
    """Carrega e divide o PDF em chunks."""
    if not os.path.exists(os.getenv('PDF_PATH')):
        raise FileNotFoundError(f"Arquivo {os.getenv('PDF_PATH')} n√£o encontrado")

    text = ""
    with open(os.getenv('PDF_PATH'), "rb") as f:
        pdf = PdfReader(f)
        for page in pdf.pages:
            text += page.extract_text() or ""
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    return text_splitter.split_text(text)

def get_vector_store():
    """Gerencia o banco vetorial FAISS."""
    embeddings = HuggingFaceEmbeddings(model_name=os.getenv('EMBEDDINGS_MODEL'))
    if os.path.exists(os.getenv('VECTOR_STORE_PATH')):
        return FAISS.load_local(
            os.getenv('VECTOR_STORE_PATH'), 
            embeddings, 
            allow_dangerous_deserialization=True
        )
    else:
        chunks = load_and_split_pdf()
        vector_store = FAISS.from_texts(chunks, embeddings)
        vector_store.save_local(os.getenv('VECTOR_STORE_PATH'))
        return vector_store

def init_rag_chain():
    """Inicializa a cadeia RAG."""
    llm = Ollama(model=os.getenv('MODEL_NAME'))
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=get_vector_store().as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

rag_chain = init_rag_chain()

@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        prompt = data.get("prompt", "").strip()
        
        if not prompt:
            return jsonify({"error": "Prompt vazio"}), 400

        # Verifica sauda√ß√µes
        for pattern, response in RESPONSE_TEMPLATES["greetings"].items():
            if re.fullmatch(pattern, prompt):
                return jsonify({
                    "response": response(re.match(pattern, prompt)) if callable(response) else response,
                    "sources": []
                })

        # Processa com RAG
        result = rag_chain({"query": prompt})
        
        # Verifica resposta irrelevante
        if any(phrase in result["result"].lower() for phrase in ["n√£o sei", "n√£o encontro", "n√£o tenho informa√ß√µes"]):
            return jsonify({
                "response": RESPONSE_TEMPLATES["fallback"],
                "sources": []
            })
        
        return jsonify({
            "response": result["result"],
            "sources": [{"page": i+1} for i, doc in enumerate(result["source_documents"])]
        })

    except Exception as e:
        return jsonify({"error": f"Erro: {str(e)}"}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=5000)